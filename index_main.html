<!DOCTYPE HTML>
<html>
	<head>
		<!--<link href="Lab/images/main/ewhaMark.png" rel="shortcut icon" type="image/x-icon">-->
		<link href="Lab/images/main/ipcalLogo2.jpg" rel="shortcut icon" type="image/x-icon">
		<title>IP-CAL</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="Lab/assets/css/main.css" />
	</head>
	<body>
		<div id="page-wrapper">
			<!-- Header -->
				<div id="header-wrapper">
					<div class="container">
						<div class="row">
							<div class="col-12">
								<header id="header">
									<h1><a href="index.html" id="logo">IP-CAL</a></h1>
									<nav id="nav">
										<a href="index.html" class="current-page-item">HOME</a>
										<a href="Lab/member.html">MEMBER</a>
										<a href="Lab/publication.html">PUBLICATION</a>
										<a href="Lab/project.html">PROJECT</a>
										<a href="Lab/class.html">CLASS</a>
										<a href="Lab/pictures.html">PHOTO</a>
										<a href="Lab/contact.html">CONTACT</a>
									</nav>
								</header>

							</div>
						</div>
					</div>
				</div>

			<!-- Main -->
				<div id="main">
					<div class="container">
						<div class="row main-row">
							<div class="col-8 col-12-medium">
								<section>
									<h2>Intelligence System and Parallel Computer Architecture Lab </h2>
									<p><img src="Lab/images/main/ipcalLogo2.jpg" class="left" width="140" height="140"/>
									<!--<img src="Lab/images/main/ewhaMark.png" class="left" width="120" height="120" />-->
									Intelligence system and Parallel Computer Architecture (IP-CAL) Lab was founded in March 2021. Our lab is located in Ewha Womans University. Our research interests are Graphics Processing Units (GPU), machine learning accelerators, parallel programming, and computer architecture. Detailed research topics are described below. If you are interested in the topics, you are always welcome to visit our lab. 
									</p>

								</section>
								<hr>
								<section>
								<h2>Graphics Processing Unit (GPU) Micro-Architecture</h2>
								<p>
									<img src="Lab/images/main/gpuArchitecture.png" class="left" width="250" />
									GPUs were first developed to accelerate graphics applications. Games are one major application that relies on the performance of GPUs. When the game application is launched, GPUs start to create 60~120 images every second using a massive number of GPU processors. This massive number of processors recently has begun to be used for general purpose applications such as machine learning algorithms, simulations, and many other applications instead of graphics applications. This paradigm is known as General-Purpose Computing on Graphics Processing Unit (GPGPU).
								</p>
								<p>
									Our goal is to maximize the performance of GPUs when general-purpose applications such as machine learning algorithms, simulations, and many more are executed on the GPU hardware. We first identify the bottleneck points on the GPU architecture and propose a new modified architecture that can remove the bottleneck points. To verify our ideas, C/C++/Python based cycle-accurate simulations are used.
								</p>

								</section>
								<hr>
								<section>
								<h2>Ray Tracing Accelerator (RTA)</h2>
								<p>
									<img src="Lab/images/main/rta.png" class="left" width="350" />
									Ray tracing is a rendering technique that creates photorealistic images by simulating the physics of light rays as they travel and interact with objects in a 3D scene. It is widely used in high-end applications such as movies and Computer-Aided Design (CAD) and has recently gained popularity in real-time applications like video games. To enable real-time ray tracing, modern Graphics Processing Units (GPUs) feature specialized Ray Tracing Accelerators (RTAs) that efficiently handle tasks such as traversal and intersection.
								</p>
								<p>
									Our goal is to enhance GPU performance for ray tracing applications. We begin by analyzing the distinct characteristics of ray tracing workloads and identifying bottlenecks in GPU architecture, especially within RTAs. Based on the analysis, we propose an optimized architecture to improve ray tracing performance. To validate our approach, we use C/C++/CUDA based cycle-level simulators that support Vulkan, an industry-standard graphics API with ray tracing capabilities.
								</p>
								<hr>
								<section>
								<h2>Machine Learning Accelerator</h2>
								<p>
									<img src="Lab/images/main/systolicArray.png" class="left" width="250" />
									Machine learning algorithms have been applied in various areas such as image recognition, voice speech recognition, translation, text classification, and more. These algorithms are traditionally operated on Central Processing Units (CPUs) and Graphics Processing Units (GPUs). Especially, GPUs are widely used for the execution of applications. However, CPUs and GPUs are not designed for machine learning algorithms, there have been several issues in terms of performance and power consumption. To resolve the problems, various machine learning accelerator designs have been proposed by researchers. One famous design is using a systolic array that has many small processing units which are only designed to perform Multiply-And-Accumulate (MAC) operations. These small processing units are connected only to their neighbor so that the data can be transferred from one processing unit to the other processing unit.</p>
								<p>Our goal is to analyze the newly proposed machine learning accelerators. By doing this, we can find the performance bottleneck points or can detect the unnecessary processing execution cycles. Based on our analysis, we can propose advanced hardware accelerators for machine learning applications.</p>
								</section>
								<hr>
								<section>
								<h2>Parallel Programming</h2>
								<p>
									<img src="Lab/images/main/vectorExecution.png" class="left" width="350" />
									Single Instruction Multiple Data (SIMD) and Single Instruction Multiple Threads (SIMT) are execution models used in parallel computing. In these models, multiple threads (data) are executed in lock-step. These models are widely used for supercomputers because of their efficiency.
								</p>
								<p>
								Intel and AMD CPUs have vector processors (SIMD) that can be used by Advanced Vector Extension (AVX) instructions. In the case of NVIDIA GPUs, Compute Unified Device Architecture (CUDA) allows software developers to use massively parallel SIMT processors for general purpose applications. The developers must have a decent knowledge of the vector processing units in order to create efficient applications. Our goal is to provide proper knowledge to software developers so that the developers can create efficient programs.
								</p>
								</section>
								<hr>
								<section>
								<h2>Computer Architecture</h2>
								<p>
								Computer Architecture is a set of rules which state how hardware is connected together in order to compute complicated applications. Researchers have proposed many different techniques such as branch predictions, speculative execution, out-of-order execution, memory pre-fetching, and more. We use C/C++/Python based cycle-accurate simulations to study the previously proposed techniques.
								</p>
								</section>
								<hr>
							</div>
							<div class="col-4 col-12-medium">

								<section>
									<h2>IP-CAL News!</h2>
									<ul class="small-image-list">
										<li>
											<h4><span style="font-weight:bold;color:black;">[2025. 10.] Recruiting</span></h4>
											<p>We are currently recruiting undergraduate and graduate students who are interested in parallel programming and computer architecture. For undergraduate students, we are only considering those who are committed to continuing research in our lab as graduate students. If you are interested, please contact Prof. Yoon directly.</p>
											<h4><span style="font-weight:bold;color:black;">[2025. 08.] Congratulation</span></h4>
											<p>The papers titled “Understanding Distributed Training of Large Language Models with Unified Virtual Memory” (Jane, Eunbi) and “HALO: Hybrid Systolic Arrays via Logical Partitioning for Acceleration of Complex-Valued Neural Networks” (Ji Yeong, Eunbi, SungHee, Jane) have both been accepted to IISWC 2025.</p>
											<h4><span style="font-weight:bold;color:black;">[2025. 05.] Welcome</span></h4>
											<p>SeJin Park has joined our group as undergraduate interns.</p>
											<h4><span style="font-weight:bold;color:black;">[2025. 04.] Congratulation</span></h4>
											<p>Jiyeong has been selected as the best performer in the AI Convergence Project.</p>
											<h4><span style="font-weight:bold;color:black;">[2025. 03.] Congratulation</span></h4>
											<p>The paper titled "Hierarchical Traversal Stack Design Using Shared Memory for GPU Ray Tracing" by Eunsoo and Eunbi has been accepted to ISPASS 2025.</p>
											<h4><span style="font-weight:bold;color:black;">[2025. 02.] Congratulation</span></h4>
											<p>"Beyond VABlock: Improving Transformer Workloads through Aggressive Prefetching" by Jane and Ikyoung has been accepted in the Journal of Systems Architecture (JSA).</p>
											<h4><span style="font-weight:bold;color:black;">[2025. 01.] Welcome</span></h4>
											<p>Ga In Jeong and SungHee Yum have joined our group as undergraduate interns.</p>
											<h4><span style="font-weight:bold;color:black;">[2024. 11.] Congratulation</span></h4>
											<p>The paper titled "Warped-Compaction: Maximizing GPU Register File Bandwidth Utilization via Operand Compaction" by Eunbi has been accepted to HPCA 2025.</p>
											<h4><span style="font-weight:bold;color:black;">[2024. 09.] Welcome</span></h4>
											<p>Jane Rhee, Eunbi Jeong, Seonwoo Kim have started their master's degree.</p>
											<h4><span style="font-weight:bold;color:black;">[2024. 03.] Welcome</span></h4>
											<p>Eun Soo Jung and Jiyeong Yi have started their master's degree.</p>
											<h4><span style="font-weight:bold;color:black;">[2024. 02.] Congratulation</span></h4>
											<p>"Conflict-Aware Compiler for Hierarchical Register File on GPUs" by Eunbi and Eun Seong has been accepted in the Journal of Systems Architecture (JSA).</p>
											<h4><span style="font-weight:bold;color:black;">[2024. 01.] Welcome</span></h4>
											<p>Jiyeong Yi has joined our group as undergraduate interns.</p>
											<h4><span style="font-weight:bold;color:black;">[2023. 08.] Congratulation</span></h4>
											<p>Eunbi's first co-authored paper, "Triple-A," has been accepted in IEEE Embedded System Letters (ESL).</p>
											<h4><span style="font-weight:bold;color:black;">[2023. 07.] Congratulation</span></h4>
											<p>Eun Seong and Eunbi are honored with the Paper Award (우수논문상) at the 2023 Summer Annual Conference of IEIE.</p>
											<h4><span style="font-weight:bold;color:black;">[2023. 07.] Welcome</span></h4>
											<p>Ikyoung Choi has joined our group as undergraduate interns.</p>
											<h4><span style="font-weight:bold;color:black;">[2023. 04.] Welcome</span></h4>
											<p>Seonwoo Kim has joined our group as undergraduate interns.</p>
											<h4><span style="font-weight:bold;color:black;">[2023. 01.] Welcome</span></h4>
											<p>Jae Eun Hwang and Eunbi Jeong have joined our group as undergraduate interns.</p>
											<h4><span style="font-weight:bold;color:black;">[2022. 01.] Welcome</span></h4>
											<p>Eun Soo Jung, Yeonhee Jung, and Eun Seong Park have joined our group as undergraduate interns.</p>
											<h4><span style="font-weight:bold;color:black;">[2021. 07.] Welcome</span></h4>
											<p>Minyoung Lee, Jane Rhee, and Myeong Ji Kim have joined our group as undergraduate interns.</p>
										</li>
									</ul>
								</section>
							</div>
						</div>
					</div>
				</div>

			<!-- Footer -->
				<div id="footer-wrapper">
					<div class="container">
						<div class="row">
							<div class="col-12">
								<div id="copyright">
									&copy; <a href="https://ip-cal.ewha.ac.kr/mediawiki">IP-CAL.</a> All rights reserved. | Design: <a href="http://html5up.net">HTML5 UP</a>
								</div>
							</div>
						</div>
					</div>
				</div>

		</div>

		<!-- Scripts -->
			<script src="Lab/assets/js/jquery.min.js"></script>
			<script src="Lab/assets/js/browser.min.js"></script>
			<script src="Lab/assets/js/breakpoints.min.js"></script>
			<script src="Lab/assets/js/util.js"></script>
			<script src="Lab/assets/js/main.js"></script>

	</body>
</html>
